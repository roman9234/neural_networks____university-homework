Понятия


Глубокие сверточные нейронные сети (CNN)




Применяем:
---- Архитектура ResNet (ссылка): ----

У всех моделей с увеличением глубины улучшаются показатели
Но типо неужели можно бесконечно стакать уровни?
Они хз так как была проблема с vanishing/exploding gradients
 - Это проблема, возникающая при обучении очень глубоких нейронных сетей. Градиенты (ошибки, которые передаются обратно через сеть при обратном распространении ошибки) могут либо становиться слишком маленькими (vanishing), либо слишком большими (exploding).
Эту проблему решили за счёт:

- Normalized initialization: Это техника инициализации весов сети таким образом,
чтобы значения активаций и градиентов оставались в разумных пределах.
Например, популярная инициализация Xavier или He Initialization (используется в ResNet).
Она помогает избежать взрывов или исчезновения градиентов с самого начала обучения.

- Intermediate normalization layers: Использование слоёв нормализации, таких как Batch Normalization (BatchNorm), помогает стабилизировать обучение.
BatchNorm нормализует значения активаций внутри слоя, что предотвращает их "перекос" и упрощает обучение.


С более глубокими сетями появилась проблема ДЕГРАДАЦИИ
точнсть падает с глубиной быстро
И эта проблема не связана с переобучением

Они решают проблему создав deep residual learning framework
Что-то придумали с маппингом
risidual mapping проще оптимизировать чем то что было до того

---- Главное что есть в ResNet:
- residual connections (shortcut connections)
- Identity mapping - для упрощения обучения.
- Bottleneck архитектура - для повышения эффективности.
- Batch Normalization - для стабилизации обучения глубокой сети.



Кратко по статье:
вместо того, чтобы учить нейросеть напрямую преобразовывать входные данные  x  в выходные  H(x) ,
ResNet учит сеть предсказывать разницу (или остаток) между входом и выходом:  F(x) = H(x) - x .
Где  F(x)  — это "residual" (остаточная функция). Такой подход упрощает обучение глубоких сетей, так как легче учить небольшие изменения (остатки), чем полное преобразование.

Shortcut connections — это прямые соединения между входом слоя и его выходом. Вместо того чтобы пропускать данные только через последовательность нелинейных преобразований (например, свёртки + ReLU), shortcut connection добавляет вход  x  к выходу  F(x) :

Identity mapping — это когда residual connection добавляет входные данные напрямую к выходу без изменений
Это помогает сохранить исходную информацию от входа на всех уровнях сети. Таким образом, даже если остаточная функция  F(x)  ничего не выучит (например, все веса будут равны нулю), сеть всё равно сможет передавать информацию "как есть". Это предотвращает деградацию производительности при увеличении глубины.

Bottleneck архитектуры — это оптимизация структуры ResNet для уменьшения вычислительных затрат при сохранении производительности. Они используют комбинацию слоёв

Свёртки  3 × 3  считаются оптимальными для большинства задач компьютерного зрения. Они достаточно малы для уменьшения вычислительных затрат, но при этом позволяют эффективно захватывать локальные особенности изображения (например, текстуры или границы).






Конфигурация обучена на наборе данных:
---- ImageNet (ссылка) ----
While It's pretty easy for people to identify subtle differences in photos, computers still have a ways to go. Visually similar items are tough for computers to count


Для запуска прогнозов используем:
---- Датасет CIFAR-10 (ссылка) ----

The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class.
There are 50000 training images and 10000 test images.


This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a "fine" label (the class to which it belongs) and a "coarse" label (the superclass to which it belongs).


ResNet-50





Метрики точности:
precision
recall
f1-score









