{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb93457",
   "metadata": {},
   "source": [
    "## Методические указания по выполнению лабораторной работы №5\n",
    "\n",
    "**Тема: Обучение модели YOLO на кастомном датасете и исследование влияния гиперпараметров на качество детекции**\n",
    "\n",
    "**Цель работы:** Познакомиться с архитектурой YOLO на примере проверки гипотезы о релевантной метрике.\n",
    "\n",
    "**Задачи:**\n",
    "\n",
    "- Ознакомиться с архитектурой YOLO.\n",
    "- Изучить метрики для анализа производительности модели, выбрать целевую метрику в соответствии с вариантом.\n",
    "- Выбрать предметную область, сформировать гипотезу для проведения исследования.\n",
    "- Собрать и проаннотировать данные, сформировать датасет.\n",
    "- Провести fine-tuning предобученной модели YOLOv11 Nano/Small.\n",
    "- Визуализировать и проанализировать результаты.\n",
    "- На основе анализа сделать корректировку гиперпараметров/данных и провести вторую итерацию для повышения показателей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9931204b",
   "metadata": {},
   "source": [
    "### 1. Подготовка к обучению\n",
    "\n",
    "#### 1.1 Метрики\n",
    "\n",
    "Вариант 1 - Precision\n",
    "\n",
    "#### 1.2 Гипотеза\n",
    "\n",
    "Гипотеза должна отражать характер уклона исследования с обоснованием, отталкиваясь от предметной области. *Одна лишь констатация необходимости достижения высокого значения целевой метрики не является обоснованием*.\n",
    "\n",
    "#### 1.3 Данные\n",
    "\n",
    "Соберите не менее 500 изображений из открытых источников. Можно пользоваться готовыми наборами данных, но важно проверить качество: разрешение изображений, качество аннотаций, баланс классов. При самостоятельном сборе данных можете воспользоваться терминальной утилитой ffmpeg для нарезки видео на кадры и любым удобным инструментом аннотирования (Roboflow, CVAT и тд). \n",
    "\n",
    "#### 1.4 Предобработка\n",
    "\n",
    "Примените методы аугментации к данным для расширения объема датасета для получения 1.5-2к изображений. Подготовьте данные к требуемому формату для обучающего процесса."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f91f75c",
   "metadata": {},
   "source": [
    "### 2. Обучение модели\n",
    "\n",
    "#### 2.1 Подготовка окружения\n",
    "\n",
    "Установите зависимости и библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "id": "6441a4bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:17:28.823648Z",
     "start_time": "2025-05-19T19:17:28.820240Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from contextlib import redirect_stdout"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "e9b8ad59",
   "metadata": {},
   "source": [
    "#### 2.2 Подготовка модели\n",
    "\n",
    "Загрузите предобученную модель, определите устройство, переведите модель в режим инференса. Не используйте размер модели больше чем Small для достижения лучших показателей на стандартных гиперпараметрах (особенно imgsz)"
   ]
  },
  {
   "cell_type": "code",
   "id": "ad5ffb38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:17:28.922021Z",
     "start_time": "2025-05-19T19:17:28.829660Z"
    }
   },
   "source": [
    "# импорт модели\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = YOLO(\"yolo11_I.pt\", verbose=False).to(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:17:29.002368Z",
     "start_time": "2025-05-19T19:17:28.998086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)  # Проверка версии CUDA\n",
    "print(torch.cuda.is_available())  # Проверка доступности GPU"
   ],
   "id": "d4c9aa925bf39d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126\n",
      "12.6\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "5f2bcad1",
   "metadata": {},
   "source": [
    "#### 2.3 Загрузка и предобработка изображений\n",
    "\n",
    "\n",
    "Затем импортируйте датасет в проект и выполните трансформацию данных (при использовании Roboflow трансформация выполняется на этапе предобработки):"
   ]
  },
  {
   "cell_type": "code",
   "id": "f1ae375c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:17:29.081260Z",
     "start_time": "2025-05-19T19:17:29.077717Z"
    }
   },
   "source": [
    "# загрузка датасета\n",
    "data_path = \"C:/programming_HUB/MTUCI_projects/6_sem/neural_networks____university-homework/5_lab/dataset/dataset.yaml\"\n",
    "global_epochs = 10\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "0b60f68a",
   "metadata": {},
   "source": [
    "#### 2.4 Обучение, оценка модели и визуализация результатов\n",
    "\n",
    "Проведите обучение модели, проанализируйте кривые обучения, метрики и тестовые данные. Сделайте вывод и корректироваки для достижения лучших показателей"
   ]
  },
  {
   "cell_type": "code",
   "id": "9de59854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:17:34.172941Z",
     "start_time": "2025-05-19T19:17:29.156590Z"
    }
   },
   "source": [
    "# обучение, оценка, визуализация\n",
    "%matplotlib inline\n",
    "\n",
    "params = {\n",
    "    'data': data_path,\n",
    "    'epochs': global_epochs,\n",
    "    'imgsz': 640,\n",
    "    'batch': 16,\n",
    "    'device': device,\n",
    "    'workers': 4,\n",
    "    'optimizer': 'AdamW',\n",
    "    'seed': 42\n",
    "}\n",
    "# Обучение модели\n",
    "\n",
    "results = model.train(**params)\n",
    "\n",
    "# Оценка модели на тестовых данных\n",
    "metrics = model.val()\n",
    "model.save('yolo11_I.pt')\n",
    "\n",
    "# Визуализация результатов обучения\n",
    "\n",
    "# Получаем историю обучения из CSV файла\n",
    "# Путь к файлу с результатами\n",
    "results_csv_path = os.path.join(model.trainer.save_dir, 'results.csv')\n",
    "\n",
    "# Загружаем данные\n",
    "try:\n",
    "    results_df = pd.read_csv(results_csv_path)\n",
    "\n",
    "    # Графики метрик\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(results_df['metrics/precision(B)'], label='Precision')\n",
    "    plt.plot(results_df['metrics/recall(B)'], label='Recall')\n",
    "    plt.plot(results_df['metrics/mAP50(B)'], label='mAP@0.5')\n",
    "    plt.title('Метрики во время обучения')\n",
    "    plt.xlabel('Эпоха')\n",
    "    plt.ylabel('Значение')\n",
    "    plt.legend()\n",
    "\n",
    "    # Графики потерь\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(results_df['train/box_loss'], label='Train Box Loss')\n",
    "    plt.plot(results_df['val/box_loss'], label='Val Box Loss')\n",
    "    plt.title('Графики потерь')\n",
    "    plt.xlabel('Эпоха')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Файл с результатами не найден\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.140 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.134  Python-3.12.4 torch-2.6.0+cu126 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\u001B[34m\u001B[1mengine\\trainer: \u001B[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:/programming_HUB/MTUCI_projects/6_sem/neural_networks____university-homework/5_lab/dataset/dataset.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train45, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\programming_HUB\\MTUCI_projects\\6_sem\\neural_networks____university-homework\\runs\\detect\\train45, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mchecks passed \n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mFast image access  (ping: 0.10.0 ms, read: 909.7269.4 MB/s, size: 101.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning C:\\programming_HUB\\MTUCI_projects\\6_sem\\neural_networks____university-homework\\5_lab\\dataset\\labels\\train.cache... 355 images, 646 backgrounds, 0 corrupt: 100%|██████████| 1001/1001 [00:00<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:17:34.174951Z",
     "start_time": "2025-05-19T19:05:40.823708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"map: {metrics.box.map}\")\n",
    "plt.show()"
   ],
   "id": "f790f8d91c1dc78e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map: 0.615027953213465\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:17:34.174951Z",
     "start_time": "2025-05-19T19:05:40.836615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "del model\n",
    "import gc\n",
    "gc.collect()"
   ],
   "id": "9c15d4b59c807631",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50491"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "6b119dba",
   "metadata": {},
   "source": [
    "#### 2.5 Вторая итерация\n",
    "\n",
    "Проведите процедуры для достижения высоких показателей (корректировка данных/гиперпараметров), сделайте вывод\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cb58b167",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "model = YOLO(\"yolo11_I.pt\", verbose=False).to(device)\n",
    "\n",
    "params_v2 = {\n",
    "    'data': data_path,\n",
    "    'epochs': global_epochs,\n",
    "    'imgsz': 640,\n",
    "    'batch': 16,\n",
    "    'device': device,\n",
    "    'workers': 4,\n",
    "    'optimizer': 'AdamW',\n",
    "    'seed': 42,\n",
    "\n",
    "    # 'hsv_h': 0.3,\n",
    "    # 'hsv_s': 0.7,\n",
    "    # 'hsv_v': 0.4,\n",
    "    # 'degrees': 15,\n",
    "    # 'mixup': 0.2,\n",
    "    #\n",
    "    'lr0': 0.001,\n",
    "    'lrf': 0.01,\n",
    "    'weight_decay': 0.0005,\n",
    "    # 'warmup_epochs': 3,\n",
    "    # 'box': 7.5,\n",
    "    # 'cls': 0.5,\n",
    "    # 'dfl': 1.5,\n",
    "}\n",
    "\n",
    "# Повторное обучение модели с новыми параметрами\n",
    "print(\"\\nНачинаем вторую итерацию обучения с улучшенными параметрами\")\n",
    "results_v2 = model.train(**params_v2)\n",
    "\n",
    "# Оценка новой модели\n",
    "print(\"\\nОценка улучшенной модели.\")\n",
    "metrics_v2 = model.val()\n",
    "\n",
    "# Сохранение улучшенной модели\n",
    "model.save('yolo11_II.pt')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:29:00.367180Z",
     "start_time": "2025-05-20T06:28:59.951429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if 'metrics_v2' not in locals():\n",
    "    import torch\n",
    "    from ultralytics import YOLO\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    model = YOLO('yolo11_II.pt').to(device)\n",
    "    metrics_v2 = model.val()\n",
    "\n",
    "if 'metrics' not in locals():\n",
    "    import torch\n",
    "    from ultralytics import YOLO\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    model = YOLO('yolo11_I.pt').to(device)\n",
    "    metrics = model.val()\n",
    "\n",
    "\n",
    "def format_metric(metric):\n",
    "    \"\"\"Форматирует метрику, которая может быть numpy array или float\"\"\"\n",
    "    if hasattr(metric, '__len__'):\n",
    "        return f\"{metric[0]:.4f}\" if len(metric) > 0 else \"N/A\"\n",
    "    return f\"{metric:.4f}\"\n",
    "\n",
    "print(\"\\nМетрики после второй итерации:\")\n",
    "\n",
    "print(f\"Precision: {format_metric(metrics_v2.box.p)} (было {format_metric(metrics.box.p)}). Разница {(format_metric(metrics.box.p)/format_metric(metrics_v2.box.p))*100-100:.2f}%\")\n",
    "\n",
    "print(f\"Recall: {format_metric(metrics_v2.box.r)} (было {format_metric(metrics.box.r)}). Разница {(format_metric(metrics.box.r)/format_metric(metrics_v2.box.r))*100-100:.2f}%\")\n",
    "\n",
    "print(f\"mAP@0.5: {format_metric(metrics_v2.box.map50)} (было {format_metric(metrics.box.map50)}). Разница {(format_metric(metrics.box.map50)/format_metric(metrics_v2.box.map50))*100-100:.2f}%\")\n",
    "\n",
    "print(f\"mAP@0.5:0.95: {format_metric(metrics_v2.box.map)} (было {format_metric(metrics.box.map)}). Разница {(format_metric(metrics.box.map)/format_metric(metrics_v2.box.map))*100-100:.2f}%\")\n",
    "\n",
    "# Визуализация результатов второй итерации\n",
    "try:\n",
    "\n",
    "    # Загрузка истории обучения\n",
    "    results_csv_v2 = os.path.join(model.trainer.save_dir, 'results.csv')\n",
    "    history_v2 = pd.read_csv(results_csv_v2)\n",
    "\n",
    "    # Настройка графиков\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # График метрик\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history_v2['epoch'], history_v2['metrics/precision(B)'], label='Precision', color='blue')\n",
    "    plt.plot(history_v2['epoch'], history_v2['metrics/recall(B)'], label='Recall', color='green')\n",
    "    plt.plot(history_v2['epoch'], history_v2['metrics/mAP50(B)'], label='mAP@0.5', color='red')\n",
    "    plt.title('Гетрики метрик второй итерации')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # График потерь\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history_v2['epoch'], history_v2['train/box_loss'], label='Train Box Loss', color='blue')\n",
    "    plt.plot(history_v2['epoch'], history_v2['val/box_loss'], label='Val Box Loss', color='orange')\n",
    "    plt.title('График потерь второй итерации')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Сравнение mAP до и после\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(['Первая версия', 'Вторая версия'],\n",
    "            [metrics.box.map50, metrics_v2.box.map50],\n",
    "            color=['blue', 'green'])\n",
    "    plt.title('Сравнение mAP@0.5')\n",
    "    plt.ylabel('mAP@0.5')\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    # Сравнение Precision-Recall\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.scatter(metrics.box.p, metrics.box.r, color='blue', s=100, label='First Iteration')\n",
    "    plt.scatter(metrics_v2.box.p, metrics_v2.box.r, color='green', s=100, label='Second Iteration')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.title('Сравнение Precision-Recall')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlim(left=0)\n",
    "    plt.xlim(right=1)\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.ylim(top=1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при визуализации результатов второй итерации: {e}\")\n",
    "\n",
    "print(\"\\nУлучшенная модель сохранена как 'yolov11n_improved.pt'\")"
   ],
   "id": "b4886ea921d3dc17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "'/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m     11\u001B[39m     device = \u001B[33m'\u001B[39m\u001B[33mcuda:0\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.cuda.is_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m     12\u001B[39m     model = YOLO(\u001B[33m'\u001B[39m\u001B[33myolo11_I.pt\u001B[39m\u001B[33m'\u001B[39m).to(device)\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m     metrics = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mval\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mformat_metric\u001B[39m(metric):\n\u001B[32m     17\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Форматирует метрику, которая может быть numpy array или float\"\"\"\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\programming_HUB\\MTUCI_projects\\6_sem\\neural_networks____university-homework\\venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:630\u001B[39m, in \u001B[36mModel.val\u001B[39m\u001B[34m(self, validator, **kwargs)\u001B[39m\n\u001B[32m    627\u001B[39m args = {**\u001B[38;5;28mself\u001B[39m.overrides, **custom, **kwargs, \u001B[33m\"\u001B[39m\u001B[33mmode\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mval\u001B[39m\u001B[33m\"\u001B[39m}  \u001B[38;5;66;03m# highest priority args on the right\u001B[39;00m\n\u001B[32m    629\u001B[39m validator = (validator \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._smart_load(\u001B[33m\"\u001B[39m\u001B[33mvalidator\u001B[39m\u001B[33m\"\u001B[39m))(args=args, _callbacks=\u001B[38;5;28mself\u001B[39m.callbacks)\n\u001B[32m--> \u001B[39m\u001B[32m630\u001B[39m \u001B[43mvalidator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    631\u001B[39m \u001B[38;5;28mself\u001B[39m.metrics = validator.metrics\n\u001B[32m    632\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m validator.metrics\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\programming_HUB\\MTUCI_projects\\6_sem\\neural_networks____university-homework\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\programming_HUB\\MTUCI_projects\\6_sem\\neural_networks____university-homework\\venv\\Lib\\site-packages\\ultralytics\\engine\\validator.py:179\u001B[39m, in \u001B[36mBaseValidator.__call__\u001B[39m\u001B[34m(self, trainer, model)\u001B[39m\n\u001B[32m    176\u001B[39m     LOGGER.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSetting batch=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.args.batch\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m input of shape (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.args.batch\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, 3, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimgsz\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimgsz\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m)\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    178\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m.args.data).split(\u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m)[-\u001B[32m1\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m {\u001B[33m\"\u001B[39m\u001B[33myaml\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33myml\u001B[39m\u001B[33m\"\u001B[39m}:\n\u001B[32m--> \u001B[39m\u001B[32m179\u001B[39m     \u001B[38;5;28mself\u001B[39m.data = \u001B[43mcheck_det_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    180\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.task == \u001B[33m\"\u001B[39m\u001B[33mclassify\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    181\u001B[39m     \u001B[38;5;28mself\u001B[39m.data = check_cls_dataset(\u001B[38;5;28mself\u001B[39m.args.data, split=\u001B[38;5;28mself\u001B[39m.args.split)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\programming_HUB\\MTUCI_projects\\6_sem\\neural_networks____university-homework\\venv\\Lib\\site-packages\\ultralytics\\data\\utils.py:392\u001B[39m, in \u001B[36mcheck_det_dataset\u001B[39m\u001B[34m(dataset, autodownload)\u001B[39m\n\u001B[32m    377\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcheck_det_dataset\u001B[39m(dataset, autodownload=\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[32m    378\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    379\u001B[39m \u001B[33;03m    Download, verify, and/or unzip a dataset if not found locally.\u001B[39;00m\n\u001B[32m    380\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    390\u001B[39m \u001B[33;03m        (dict): Parsed dataset information and paths.\u001B[39;00m\n\u001B[32m    391\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m392\u001B[39m     file = \u001B[43mcheck_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    394\u001B[39m     \u001B[38;5;66;03m# Download (optional)\u001B[39;00m\n\u001B[32m    395\u001B[39m     extract_dir = \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\programming_HUB\\MTUCI_projects\\6_sem\\neural_networks____university-homework\\venv\\Lib\\site-packages\\ultralytics\\utils\\checks.py:543\u001B[39m, in \u001B[36mcheck_file\u001B[39m\u001B[34m(file, suffix, download, download_dir, hard)\u001B[39m\n\u001B[32m    541\u001B[39m files = glob.glob(\u001B[38;5;28mstr\u001B[39m(ROOT / \u001B[33m\"\u001B[39m\u001B[33m**\u001B[39m\u001B[33m\"\u001B[39m / file), recursive=\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m glob.glob(\u001B[38;5;28mstr\u001B[39m(ROOT.parent / file))  \u001B[38;5;66;03m# find file\u001B[39;00m\n\u001B[32m    542\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m files \u001B[38;5;129;01mand\u001B[39;00m hard:\n\u001B[32m--> \u001B[39m\u001B[32m543\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m does not exist\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    544\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(files) > \u001B[32m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m hard:\n\u001B[32m    545\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mMultiple files match \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, specify exact path: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfiles\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: '/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml' does not exist"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
